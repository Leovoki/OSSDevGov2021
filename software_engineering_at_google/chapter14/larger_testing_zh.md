## 被测系统
大型测试的一个关键组成部分就是前述的SUT（见图14-5）。 典型的单元测试将注意力集中在一个类或模块上。 而且，测试代码与被测试的代码在相同的进程（或Java情况下为Java虚拟机[JVM]）中运行。 对于较大的测试，SUT通常非常不同。 一个或多个带有测试代码的单独进程经常（但不总是）在其自己的进程中。
*图14-5.被测系统示例*
在Google，我们使用许多不同形式的SUT，SUT的范围是大型测试本身范围的主要驱动力之一（SUT越大，测试越大）。 可以根据两个主要因素来判断每种SUT形式：
*气密性*
这是SUT与正在使用的产品以及与所测试以外的其他组件之间的交互作用的隔离。 具有高度气密性的SUT暴露于并发源和基础架构脆弱性的可能性最小。
*保真度*
SUT反映测试中的生产系统的准确性。 具有高保真度的SUT将包含类似于生产版本的二进制形式（依赖于相似的配置，使用相似的基础结构并且具有相似的整体拓扑）。
通常来说这两个因素会有矛盾。
以下是部分SUT的实例：
*单进程SUT*
整个被测系统都打包为一个二进制文件（即使在生产中它们是多个单独的二进制文件）。 此外，可以将测试代码打包到与SUT相同的二进制文件中。 如果所有内容都是单线程的，则这种测试-SUT组合可能是“小型”测试，但对生产拓扑和配置的可信度最低。
*单机SUT*
被测系统由一个或多个单独的二进制文件（与生产系统相同）组成，并且测试是其自己的二进制文件。 但是一切都在一台机器上运行。 这用于“中等”测试。 理想情况下，我们在本地运行这些二进制文件时使用每个二进制文件的生产启动配置，以提高保真度。
*多机SUT*
被测系统分布在多台机器上（非常像生产云部署）。 与单机SUT相比，它的保真度甚至更高，但是它的使用使测试变得“大”，并且这种组合容易受到网络和机器脆弱性的影响。
*共享环境（暂存和生产）*
该测试仅使用共享环境，而不是运行独立的SUT。这样做成本最低，因为这些共享环境通常已经存在，但是该测试可能与其他同时使用发生冲突，因此必须等待将代码推送到这些环境中。 。 生产还增加了最终用户影响的风险。
*混合情况*
一些SUT表示混合：可能可以运行某些SUT，但可以使其与共享环境交互。 通常，要测试的事物是显式运行的，但其后端是共享的。 对于一家像Google这样规模庞大的公司来说，实际上不可能运行所有Google互连服务的多个副本，因此需要某种混合。

### 密封式SUT的好处
大型测试中的SUT可能是不可靠和周转时间长的主要来源。例如，生产测试使用实际的生产系统部署。 如前所述，这是流行的，因为环境没有额外的开销成本，但是在代码到达该环境之前无法运行生产测试，这意味着这些测试本身无法阻止代码向该环境的发布，也即SUT为时已晚。
最常见的第一种选择是创建一个巨大的共享登台环境并在其中运行测试。 这通常是在某些发行促进过程中完成的，但它再次将测试执行限制为仅在代码可用时执行。作为替代方案，一些团队将允许工程师在临时环境中“保留”时间，并使用该时间窗口来部署待处理的代码并运行测试，但这并不能随着越来越多的工程师或越来越多的服务而扩展 ，因为环境，其用户数量以及用户冲突的可能性都在迅速增长。
下一步是支持云隔离的或机器密封的SUT。这样的环境通过避免冲突和代码发布的保留要求来改善情况。
***
## 案例研究：在生产和Webdriver Torso中进行测试的风险
我们提到在生产中进行测试可能会带来风险。生产中的测试导致的一个幽默事件被称为Webdriver Torso事件。我们需要一种方法来验证YouTube产品中的视频渲染是否正常地创建了自动脚本来生成测试视频，上传它们并验证上传的质量 这是在Google拥有的YouTube频道Webdriver Torso中完成的。 但是这个频道和大多数视频都是公开的。
随后，该频道在《连线》上的一篇文章中进行了宣传，导致该频道在媒体中广泛传播，并为解决这个谜团做出了后续努力。 最后，博主将所有内容都归还给Google。最终，我们通过玩一些有趣的游戏而变得干净，包括Rickroll和Easter Egg，因此一切工作都很好。但是我们确实需要考虑最终用户发现我们生产中包含的任何测试数据并为此做好准备的可能性。
***
### 在边界减小SUT大小
有一些特别不友好的测试边界可以避免。涉及前端和后端的测试十分痛苦，因为众所周知，用户界面（UI）测试不可靠且成本很高。
* UI经常变化外观，这使UI测试变难，但实际上并没有影响基础行为。
* UI通常具有难以测试的异步行为。 

尽管对服务的UI进行端到端测试一直到后端都很有用，但是这些测试对UI和后端都有成倍的维护成本。 相反，如果后端提供了公共API，则通常更容易在UI / API边界将测试分为连接的测试，并使用公共API驱动端到端测试。 无论UI是浏览器，命令行界面（CLI），桌面应用程序还是移动应用程序，这都是正确的。
第三方依赖项是另一个特殊的边界。 第三方系统可能没有用于测试的公共共享环境，在某些情况下，将流量发送给第三方会产生一定的成本。 因此，不建议让自动化测试使用真正的第三方API，并且这种依赖关系是拆分测试的重要缝隙。
为了解决此大小问题，我们通过用内存数据库替换其数据库并删除了我们实际关心的SUT范围之外的其中一台服务器，从而使该SUT变得更小，如图14-6所示。 该SUT更有可能安装在一台机器上。
*图14-6.SUT裁剪版*
关键在于确定保真度与成本/可靠性之间的权衡，并确定合理的界限。 如果我们可以运行少量的二进制文件并进行测试，然后将其全部打包到执行常规编译，链接和单元测试执行的同一台计算机中，那么对于我们的工程师而言，我们将拥有最简单，最稳定的“集成”测试。
### 记录/重播的代理
在上一章中，我们讨论了测试加倍和方法，这些方法可用于将被测类与其难以测试的依赖项分离开来。我们还可以通过使用具有等效API的模拟，存根或伪造服务器或进程来使整个服务器和进程加倍。但是，不能保证所使用的测试倍数实际上符合它要替换的真实物品的协议。
处理SUT的依存于附属服务的一种方法是使用双重测试，但人们如何知道双重反映依存关系的实际行为呢？ 在Google之外，一种日益增长的方法是使用一个框架来进行消费者驱动的合同测试。这些是为客户和服务提供者定义合同的测试，并且该合同可以推动自动化测试。也就是说，客户端定义了服务的模拟，也即对于这些输入参数，我得到特定的输出。然后，真实服务在真实测试中使用该输入/输出对，以确保在给定这些输入的情况下产生该输出。消费者驱动的合同测试的两个公共工具是契约合同测试和Spring Cloud合同。Google严重依赖协议缓冲区，这意味着我们不在内部使用这些缓冲区。
在Google，我们做了一些与众不同的事情。最受欢迎的方法（有一个公共API）是使用较大的测试来生成较小的测试，即在运行较大的测试时记录到这些外部服务的流量，并在运行较小的测试时重放这些流量。较大的测试或“记录模式”测试在提交后连续运行，但是其主要目的是生成这些流量日志（必须通过测试才能生成日志）。在开发和预提交测试期间使用较小的或“重播模式”测试。
记录/重放的工作方式有趣的方面之一在于，由于不确定性，必须通过匹配器匹配请求以确定要重放的响应。这使得它们与存根和模拟非常相似，因为参数匹配用于确定结果行为。
在新的测试或客户端行为发生显著变化的测试中会发生什么情况？在这些情况下，请求可能不再与记录的流量文件中的内容匹配，因此测试无法在重播模式下通过。在这种情况下，工程师必须以“记录”模式运行测试以生成新的流量，因此使“运行”记录测试简易、快速和稳定是十分重要的。
## 测试数据
一个测试需要数据，而一个大型测试则需要两种不同的数据：
*种子数据*
预初始化到被测系统中的数据，反映了测试开始时SUT的状态。
*测试轨迹*
在测试执行期间由测试本身发送到被测系统的数据。
由于存在单独且更大的SUT的概念，因此，播种SUT状态的工作通常比在单元测试中完成的设置工作要复杂几个数量级。
例如：
*域数据*
一些数据库包含预先填充到表中并用作环境配置的数据。如果未提供域数据，则使用此类数据库的实际服务二进制文件可能会在启动时失败。
*现实的基线*
为了将SUT视为现实，可能会在启动时就质量和数量方面要求一组现实的基础数据。例如，社交网络的大型测试可能需要一个真实的社交图作为测试的基本状态：足够多的具有逼真的配置文件的测试用户以及这些用户之间必须存在足够的互连关系才能接受测试。
*播种APIs*
用来播种数据的API可能很复杂。 可能可以直接写入数据存储，但是这样做可能会绕过由执行写入操作的实际二进制文件执行的触发器和检查。
数据可以通过不同的方式生成，例如：
*手工数据*
像较小的测试一样，我们可以手动为较大的测试创建测试数据。 但是在大型SUT中为多个服务设置数据可能需要更多的工作，并且我们可能需要为大型测试创建大量数据。
*复制的数据*
我们可以复制数据，通常是从生产中复制数据。例如，我们可以通过从生产地图数据的副本开始提供基准来测试地球地图，然后测试对其所做的更改。
*样例数据*
复制数据可能会提供过多的数据，无法合理使用。采样数据可以减少数据量，从而减少测试时间并使推理更加容易。“智能采样”包括复制实现最大覆盖范围所需的最小数据的技术。
## 验证
在运行SUT并将流量发送到它之后，我们仍然必须验证行为，可以通过几种不同的方法来完成此操作：
*手工*
就像您在本地尝试二进制文件时一样，手动验证使用人工与SUT交互以确定其是否正常运行。该验证可以包括通过执行一致的测试计划中定义的操作来测试回归，也可以是探索性的，通过不同的交互路径来确定可能的新故障。
请注意，手动回归测试不能线性地扩展：系统越大，并且经过的行程越多，则需要更多的人工时间进行手动测试。
*断言*
就像单元测试一样，这些是对系统预期行为的显式检查。 例如，对于Google搜索xyzzy的集成测试，一个断言可能如下：
			assertThat(response.Contains("Colossal Cave"))
*A/B比较（差异）*
A / B测试不是定义显式的断言，而是运行SUT的两个副本，发送相同的数据并比较输出。没有明确定义预期的行为：人们必须手动检查差异以确保进行任何预期的更改。
## 大规模测试类型
现在，我们可以将这些不同的SUT，数据和断言方法结合起来，以创建不同种类的大型测试。 这样，每个测试就可以减轻哪些风险就具有不同的属性。 编写，维护和调试它需要多少工作； 以及在运行资源方面的成本。
以下是我们在Google上使用的各种大型测试的列表，它们的组成方式，服务的目的以及它们的局限性：
* 一个或多个二进制文件的功能测试
* 浏览器和设备测试
* 性能，负载和压力测试
* 部署配置测试
* 探索性测试
* A / B差异（回归）测试
* 用户验收测试（UAT）
* 探针和金丝雀分析
* 灾难恢复与混乱工程
* 用户评估


考虑到如此众多的组合，因此进行了广泛的测试，我们如何管理做什么以及何时进行？ 设计软件的一部分正在起草测试计划，而测试计划的关键部分是确定需要哪种类型的测试以及每种类型需要多少测试的战略要点。该测试策略确定了主要的风险向量以及减轻这些风险向量的必要测试方法。
在Google，我们担负着“测试工程师”的专门工程角色，而我们希望优秀的测试工程师所追求的目标之一就是能够为我们的产品制定测试策略。
## 一或多个交互二进制文件功能测试
这些类型的测试具有以下特征：
* SUT：单机密封或云部署隔离
* 数据：手工制作
* 验证：断言

到目前为止，我们已经看到，单元测试无法真正保真地测试复杂的系统，这仅仅是因为它们的打包方式与实际代码的打包方式不同。许多功能测试方案与给定的二进制文件进行交互的方式与该二进制文件内部的类的交互方式不同，并且这些功能测试需要单独的SUT，因此是规范的大型测试。
测试多个二进制文件的交互比测试单个二进制文件还要复杂的情况并不奇怪。当服务被部署为许多单独的二进制文件时，微服务环境中是一个常见的用例。在这种情况下，通过启动由所有相关二进制文件组成的SUT并通过已发布的API进行交互，功能测试可以涵盖二进制文件之间的实际交互。

## 浏览器和设备测试
测试Web UI和移动应用程序是对一个或多个交互二进制文件进行功能测试的一种特殊情况。可以对基础代码进行单元测试，但是对于最终用户而言，公共API是应用程序本身。使测试通过前端与作为第三方的应用程序进行交互可以提供额外的覆盖范围。
## 性能，负载和压力测试
这些类型的测试具有以下特征：
* SUT：云部署隔离
* 数据：来自生产的手工制作或多路复用
* 验证：差异（性能指标）


尽管可以在性能，负载和压力方面测试较小的单元，但此类测试通常需要同时向外部API发送流量。 该定义意味着此类测试是多线程测试，通常在被测二进制文件的范围内进行测试。但是，这些测试对于确保版本之间的性能不会降低以及系统可以处理预期的流量高峰至关重要。
随着负载测试规模的增长，输入数据的范围也随之增长，最终将难以生成触发负载下的错误所需的负载规模。负载和压力处理是系统的“高度紧急”特性。也就是说，这些复杂的行为属于整个系统，但不属于单个成员。因此，重要的是要使这些测试看起来尽可能地接近生产环境。每个SUT都需要与生产所需资源相似的资源，并且难以减轻生产拓扑中的噪声
消除性能测试中的噪声的一项研究领域是修改部署拓扑，即各种二进制文件如何在计算机网络中分布。运行二进制文件的计算机可能会影响性能特征；因此，如果在性能差异测试中，基本版本在快速计算机（或具有快速网络的计算机）上运行，而新版本在速度较慢的计算机上运行，则它看起来将会像是性能下降。此特征意味着最佳部署是在同一台计算机上运行两个版本。如果一台机器不能同时使用两个版本的二进制文件，则另一种方法是通过执行多次运行并消除峰和谷来进行校准。
## 部署配置测试
这些类型的测试具有以下特征：
* SUT：单机密封或云部署隔离
* 数据：无
* 验证：断言（不会崩溃）


很多时候，缺陷的来源不是代码，而是缺陷的配置：数据文件，数据库，选项定义等。 较大的测试可以测试SUT及其配置文件的集成，因为在启动给定的二进制文件期间会读取这些配置文件。这样的测试实际上是对SUT的冒烟测试，不需要太多的额外数据或验证。如果SUT成功启动，则测试通过。 如果不是，则测试失败。
## 探索性测试
这些类型的测试具有以下特征：
* SUT：生产或共享暂存
* 数据：生产或已知的测试领域
* 验证：手动


探索性测试是一种手动测试的形式，其重点不是通过重复已知的测试流程来寻找行为回归，而是通过尝试新的用户场景来寻找可疑行为。受过培训的用户/测试人员通过产品的公共API与产品进行交互，寻找通过系统的新路径，以及行为是否偏离预期或直观行为，或者是否存在安全漏洞。
探索性测试对于新系统和已发布系统都非常有用，以发现意外行为和副作用。 通过让测试人员遵循贯穿系统的不同可达路径，我们可以扩大系统覆盖范围，并在这些测试人员发现错误时捕获新的自动化功能测试。 从某种意义上讲，这有点像功能集成测试的手动“模糊测试”版本。
### 局限
手动测试不能线性扩展；也就是说，进行手动测试需要人工。探索性测试中发现的任何缺陷都应使用可以更频繁地运行的自动测试来复制。
### 漏洞
我们用于手动探索性测试的一种常见方法是bug bash。一组工程师和相关人员（经理，产品经理，测试工程师，任何熟悉产品的人员）安排了一次“会议”，但在本次会议上，每个参与人员都手动进行了产品测试。可能有一些针对特定漏洞领域和/或使用系统的出发点的已发布指南，但目标是提供足够的交互方式，以记录可疑的产品行为和彻底的错误。
## A / B差异回归测试
这些类型的测试具有以下特征：
* SUT：两个部署了云的隔离环境
* 数据：通常从生产中多路复用或采样
* 验证：A / B差异比较

单元测试涵盖了一小段代码的预期行为路径。但是，无法预测给定面向公众的产品的许多可能的故障模式。此外，正如Hyrum定律所指出的那样，实际的公共API并不是声明的一种，而是产品的所有用户可见的方面。考虑到这两个属性，A / B差异测试可能是Google进行大型测试的最常见形式，这不足为奇。从概念上讲，这种方法可以追溯到1998年。自2001年以来，我们在Google的大多数产品（从广告，搜索和地图开始）就一直基于此模型进行测试。
A / B差异测试通过将流量发送到公共API并比较新旧版本之间的响应（尤其是在迁移过程中）来进行操作。行为上的任何偏差都必须按照预期或未预期的方式进行调和（回归）。在这种情况下，SUT由两组实际二进制文件组成：一组运行在候选版本上，另一组运行在基本版本上。并且第三个二进制文件发送流量并比较结果。
当然也有其他变体。我们使用A-A测试（将系统与其自身进行比较）来识别不确定的行为，噪声和脆弱性，并帮助从A-B差异中删除那些不确定性。我们还偶尔使用ABC测试，比较最后的生产版本，基准构建和有待更改的内容。最后不仅可以看到立即更改的影响，还可以在下一个发行版本看到潜在更改的累积影响。
A / B差异测试是一种方便但可自动化的方法，用于检测已启动系统的任何异常情况。

### 局限
差异测试确实带来了一些要解决的挑战：
*赞同*
必须有人足够理解结果，才能知道于预期是否会有任何差异。 与典型的测试不同，其不清楚差异是好是坏（或基准版本实际上是否有效），因此在此过程中通常需要手动进行操作。
*噪音*
对于差异测试，将任何意外的噪声引入结果的方法都会导致对结果进行更多的人工调查。因此有必要弥补噪声，这是构建良好的差异测试的一大复杂原因。
*覆盖范围*
为差异测试生成足够的有用流量可能是一个具有挑战性的问题。 测试数据必须涵盖足够的场景以识别极端情况下的差异，但是手动整理此类数据非常困难。
*设置*
配置和维护一个SUT颇具挑战性。 一次创建两个可以使复杂度增加一倍，尤其是当它们共享相互依赖关系时。